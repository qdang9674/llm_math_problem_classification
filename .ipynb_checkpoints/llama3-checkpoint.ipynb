{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d13b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import champ_dataset\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8164f73-f927-4c4e-b13e-3e6ee35c8719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ea195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================Load the dataset==================================\n",
    "# dataset is a champ_dataset.Dataset instance; 'v0' is the dataset used in the paper\n",
    "dataset = champ_dataset.load('v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e5a5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_Inequality_19: For positive a, b, c, d, at most how many of the three inequalities a+b<c+d, (a+b)(c+d)<ab+cd, (a+b)cd<ab(c+d) can be true at the same time?\n"
     ]
    }
   ],
   "source": [
    "# =====================Get a random problem and read its content=====================\n",
    "# get a random problem; dataset.problems is a dictionary\n",
    "problem_id = random.choice(list(dataset.problems.keys()))\n",
    "# shortcut for dataset.problems[problem_id]\n",
    "problem = dataset[problem_id]\n",
    "# problem identifier (which is equal to problem_id) and problem statement\n",
    "print(f'{problem.identifier}: {problem.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567f8ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a hint: H_Inequality_19_1\n",
      "Text: Find an assignment of a, b, c that makes two inequalities true.\n",
      "--------------End of this hint---------------\n",
      "We have a hint: H_Inequality_19_2\n",
      "Text: Study whether all inequalities can be true by multiplying the first two together, and then the last two together.\n",
      "--------------End of this hint---------------\n",
      "We have a concept: C_(a+b)sq_4ab\n",
      "Text: For real numbers a, b, (a+b)^2≥4ab.\n",
      "Category: Inequality\n",
      "Parent concept (C_sumdiff_sq): (x±y)^2=x^2±2xy+y^2.\n",
      "-------------End of this concept-------------\n"
     ]
    }
   ],
   "source": [
    "# iterate over the list of relevant concepts and hints\n",
    "for ch_id in problem.ch_list:\n",
    "    ch = dataset[ch_id]  # ch is either a champ_dataset.Concept instance or champ_dataset.Hint instance\n",
    "    if isinstance(ch, champ_dataset.Concept):  # displaying a concept\n",
    "        print(f'We have a concept: {ch.identifier}')\n",
    "        print(f'Text: {ch.text}')  # content of the concept\n",
    "        print(f'Category: {ch.category}')  # category of the concept\n",
    "        if ch.name is not None:  # some concept has a name\n",
    "            print(f'Name: {ch.name}')\n",
    "        if ch.parent is not None:  # some concept has a parent concept (i.e., a more general version)\n",
    "            print(f'Parent concept ({ch.parent.identifier}): {ch.parent.text}')\n",
    "        print('-------------End of this concept-------------')\n",
    "    else:  # displaying a hint\n",
    "        print(f'We have a hint: {ch.identifier}')\n",
    "        print(f'Text: {ch.text}')  # content of the hint\n",
    "        print('--------------End of this hint---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11237a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 2 inequalities\n",
      "Step-wise soluion:\n",
      "Step 0: We can see that the first two inequalities can be satisified with a=b=1, c=d=10, as 2<20 and 40<102.\n",
      "  This step uses the following concepts and hints: H_Inequality_19_1\n",
      "Step 1: Assume that all three inequalities are true.\n",
      "  This step uses the following concepts and hints: H_Inequality_19_2\n",
      "Step 2: Multiplying the first two inequalities, we get (a+b)^2<ab+cd.\n",
      "  This step uses the following concepts and hints: H_Inequality_19_2\n",
      "Step 3: Since (a+b)^2≥4ab, we have 4ab<ab+cd, or cd>3ab.\n",
      "  This step uses the following concepts and hints: H_Inequality_19_2, C_(a+b)sq_4ab\n",
      "Step 4: Multiplying the last two inequalities, we get (a+b)^2*cd<ab(ab+cd).\n",
      "  This step uses the following concepts and hints: H_Inequality_19_2\n",
      "Step 5: Since (a+b)^2≥4ab, we have 4abcd<abab+abcd, or ab>3cd.\n",
      "  This step uses the following concepts and hints: H_Inequality_19_2, C_(a+b)sq_4ab\n",
      "Step 6: The inequalities ab>3cd and cd>3ab cannot be true at the same time for positive ab and cd.\n",
      "  This step uses the following concepts and hints: H_Inequality_19_2\n",
      "Step 7: Thus, we cannot have all three inequalities to be true at the same time.\n",
      "  This step uses the following concepts and hints: H_Inequality_19_2\n",
      "Step 8: So at most 2 inequalities can be true.\n",
      "  This step does not use any concepts or hints.\n"
     ]
    }
   ],
   "source": [
    "print(f'Answer: {problem.answer}')  # final answer\n",
    "print('Step-wise soluion:')\n",
    "# problem.solution.steps is a list of champ_dataset.Step object\n",
    "for idx, step in enumerate(problem.solution.steps):\n",
    "    print(f'Step {idx}: {step.text}')  # content of the step\n",
    "    # step.ch_idxs is the list of concepts and hints associated with this step\n",
    "    # (by their index in problem.ch_list)\n",
    "    if len(step.ch_idxs) == 0:\n",
    "        print('  This step does not use any concepts or hints.')\n",
    "    else:\n",
    "        print('  This step uses the following concepts and hints: ' + \\\n",
    "            ', '.join([problem.ch_list[idx] for idx in step.ch_idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e809e792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt generator supports the following prompts: ['0-Shot', '5-Shot', '1/3 Soln', '2/3 Soln', 'No C w/o H', 'No C w/ H', 'Direct C w/o H', 'Direct C w/ H', 'Name C w/o H', 'Name C w/ H', 'Example C w/o H', 'Example C w/ H', 'Root C w/o H', 'Root C w/ H', 'Problem C w/o H', 'Problem C w/ H', 'Misleading C w/o H', 'Misleading C w/ H']\n",
      "Generating prompt: Name C w/o H\n",
      "-----------------------\n",
      "System prompt: \n",
      "You are an expert on mathematics.\n",
      "-----------------------\n",
      "User input 1: \n",
      "Solve the following problem. Make sure to show your work before giving the final answer.\n",
      "\n",
      "For positive a, b, c, d, at most how many of the three inequalities a+b<c+d, (a+b)(c+d)<ab+cd, (a+b)cd<ab(c+d) can be true at the same time?\n",
      "\n",
      "You may find the following information useful:\n",
      "\n",
      "1. For real numbers a, b, (a+b)^2≥4ab.\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# ======================Use PromptGenerator to generate prompts======================\n",
    "generator = champ_dataset.PromptGenerator(dataset)  # the prompt generator is defined on a dataset\n",
    "print(f'The prompt generator supports the following prompts: {generator.get_all_prompt_names()}')\n",
    "# randomly select a prompt to generate\n",
    "name = random.choice(generator.get_all_prompt_names())\n",
    "print(f'Generating prompt: {name}')\n",
    "sys_prompt, user_inputs, imputed_outputs = generator.construct_prompt(name, problem)\n",
    "print('-----------------------')\n",
    "print(f'System prompt: \\n{sys_prompt}')\n",
    "print('-----------------------')\n",
    "for idx, msg in enumerate(user_inputs):\n",
    "    print(f'User input {idx+1}: \\n{msg}')\n",
    "    print('-----------------------')\n",
    "for idx, msg in enumerate(imputed_outputs):\n",
    "    print(f'Imputed output {idx+1}: \\n{msg}')\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f845b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the following FWS annotations: ['GPT-4 Turbo|No C w/o H', 'GPT-4 Turbo|Direct C w/ H']\n",
      "Displaying FWS annotation: GPT-4 Turbo|No C w/o H\n",
      "The model response is generated by GPT-4 Turbo\n",
      "Beginning of model response: Let's analyze each inequality one by one and see if we can find a relationship between them.\n",
      "\n",
      "1. \\( a + b < c + d \\)\n",
      "\n",
      "This inequality simply states that the sum of \\( a \\) and \\( b \\) is less than the\n",
      "The error happens at characters 500-562\n",
      "The text span is: Grouping terms, we get:\n",
      "\n",
      "\\( c(a + b - d) + a(d + b - c) < 0 \\)\n"
     ]
    }
   ],
   "source": [
    "# the following code is (currently) only supported on the 'v0' version of the dataset\n",
    "\n",
    "# =======================Read the first wrong step annotation========================\n",
    "# problem.fws_annotations is a dictionary; note the key format as '{model}|{prompt}'\n",
    "print(f'We have the following FWS annotations: {list(problem.fws_annotations.keys())}')\n",
    "# get a key-value pair from the dictionary\n",
    "key, annotation = next(iter(problem.fws_annotations.items()))\n",
    "print(f'Displaying FWS annotation: {key}')\n",
    "print(f'The model response is generated by {annotation.author}')\n",
    "# annotation.text is the full model-generated solution\n",
    "print(f'Beginning of model response: {annotation.text[:200]}')\n",
    "if annotation.start_idx is None:\n",
    "    print('The model-generated solution is fully correct')\n",
    "else:\n",
    "    # start_idx inclusive, end_idx exclusive\n",
    "    print(f'The error happens at characters {annotation.start_idx}-{annotation.end_idx}')\n",
    "    # shortcut for annotation.text[annotation.start_idx : annotation.end_idx]\n",
    "    print(f'The text span is: {annotation.wrong_step()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7011cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the following conversations: ['GPT-3.5|0-Shot', 'GPT-3.5|5-Shot', 'GPT-3.5|1/3 Soln', 'GPT-3.5|2/3 Soln', 'GPT-3.5|No C w/o H', 'GPT-3.5|No C w/ H', 'GPT-3.5|Direct C w/o H', 'GPT-3.5|Direct C w/ H', 'GPT-3.5|Root C w/o H', 'GPT-3.5|Root C w/ H', 'GPT-3.5|Name C w/o H', 'GPT-3.5|Name C w/ H', 'GPT-3.5|Example C w/o H', 'GPT-3.5|Example C w/ H', 'GPT-3.5|Problem C w/o H', 'GPT-3.5|Problem C w/ H', 'GPT-3.5|Misleading C w/o H', 'GPT-3.5|Misleading C w/ H', 'GPT-4|0-Shot', 'GPT-4|5-Shot', 'GPT-4|1/3 Soln', 'GPT-4|2/3 Soln', 'GPT-4|No C w/o H', 'GPT-4|No C w/ H', 'GPT-4|Direct C w/o H', 'GPT-4|Direct C w/ H', 'GPT-4|Root C w/o H', 'GPT-4|Root C w/ H', 'GPT-4|Name C w/o H', 'GPT-4|Name C w/ H', 'GPT-4|Example C w/o H', 'GPT-4|Example C w/ H', 'GPT-4|Problem C w/o H', 'GPT-4|Problem C w/ H', 'GPT-4|Misleading C w/o H', 'GPT-4|Misleading C w/ H', 'GPT-4 Turbo|0-Shot', 'GPT-4 Turbo|5-Shot', 'GPT-4 Turbo|1/3 Soln', 'GPT-4 Turbo|2/3 Soln', 'GPT-4 Turbo|No C w/o H', 'GPT-4 Turbo|No C w/ H', 'GPT-4 Turbo|Direct C w/o H', 'GPT-4 Turbo|Direct C w/ H', 'GPT-4 Turbo|Root C w/o H', 'GPT-4 Turbo|Root C w/ H', 'GPT-4 Turbo|Name C w/o H', 'GPT-4 Turbo|Name C w/ H', 'GPT-4 Turbo|Example C w/o H', 'GPT-4 Turbo|Example C w/ H', 'GPT-4 Turbo|Problem C w/o H', 'GPT-4 Turbo|Problem C w/ H', 'GPT-4 Turbo|Misleading C w/o H', 'GPT-4 Turbo|Misleading C w/ H', 'PaLM 2 Medium|0-Shot', 'PaLM 2 Medium|5-Shot', 'PaLM 2 Medium|1/3 Soln', 'PaLM 2 Medium|2/3 Soln', 'PaLM 2 Medium|No C w/o H', 'PaLM 2 Medium|No C w/ H', 'PaLM 2 Medium|Direct C w/o H', 'PaLM 2 Medium|Direct C w/ H', 'PaLM 2 Medium|Root C w/o H', 'PaLM 2 Medium|Root C w/ H', 'PaLM 2 Medium|Name C w/o H', 'PaLM 2 Medium|Name C w/ H', 'PaLM 2 Medium|Example C w/o H', 'PaLM 2 Medium|Example C w/ H', 'PaLM 2 Medium|Problem C w/o H', 'PaLM 2 Medium|Problem C w/ H', 'PaLM 2 Medium|Misleading C w/o H', 'PaLM 2 Medium|Misleading C w/ H']\n",
      "Displaying conversation: GPT-3.5|0-Shot\n",
      "Role: System\n",
      "Text: You are an expert on mathematics.\n",
      "Error: None\n",
      "-----------------------\n",
      "Role: User\n",
      "Text: Solve the following problem. Make sure to show your work before giving the final answer.\n",
      "\n",
      "For positive a, b, c, d, at most how many of the three inequalities a+b<c+d, (a+b)(c+d)<ab+cd, (a+b)cd<ab(c+d) can be true at the same time?\n",
      "Error: None\n",
      "-----------------------\n",
      "Role: Model.GPT-3.5\n",
      "Text: To solve this problem, let's consider each inequality separately.\n",
      "\n",
      "1. a+b < c+d:\n",
      "We can rewrite this inequality as (a+b) - (c+d) < 0.\n",
      "Rearranging the terms, we have a - c + b - d < 0.\n",
      "Since a, b, c, and d are positive, we can conclude that a - c < d - b.\n",
      "\n",
      "2. (a+b)(c+d) < ab+cd:\n",
      "Expanding both sides of the inequality, we get ac + ad + bc + bd < ab + cd.\n",
      "Rearranging the terms, we have ac - ab + ad - cd + bc - bd < 0.\n",
      "Simplifying further, we get a(c - b) + d(a - c) + b(c - d) < 0.\n",
      "Since a, b, c, and d are positive, we can conclude that a - c < b - d.\n",
      "\n",
      "3. (a+b)cd < ab(c+d):\n",
      "Expanding both sides of the inequality, we get acd + bcd < abc + abd.\n",
      "Rearranging the terms, we have acd - abc + bcd - abd < 0.\n",
      "Simplifying further, we get ac(d - b) + bd(c - a) < 0.\n",
      "Since a, b, c, and d are positive, we can conclude that d - b < a - c.\n",
      "\n",
      "Now, let's consider the possible combinations of the three inequalities:\n",
      "\n",
      "- If a - c < d - b and a - c < b - d, then d - b > a - c, which contradicts the third inequality. Therefore, these two inequalities cannot be true at the same time.\n",
      "\n",
      "- If a - c < d - b and d - b < a - c, then a - c > d - b, which contradicts the first inequality. Therefore, these two inequalities cannot be true at the same time.\n",
      "\n",
      "- If d - b < a - c and a - c < b - d, then b - d > a - c, which contradicts the second inequality. Therefore, these two inequalities cannot be true at the same time.\n",
      "\n",
      "Therefore, it is not possible for any two of the three inequalities to be true at the same time.\n",
      "\n",
      "Hence, at most only one of the three inequalities can be true at the same time.\n",
      "Error: None\n",
      "-----------------------\n",
      "Role: User\n",
      "Text: Now, summarize the answer above in one sentence, without any intermediate steps or explanations.\n",
      "Error: None\n",
      "-----------------------\n",
      "Role: Model.GPT-3.5\n",
      "Text: At most, only one of the three inequalities can be true at the same time.\n",
      "Error: None\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# ========================Read all model-generated solutions=========================\n",
    "# problem.conversations is a dictionary; note the key format as '{model}|{prompt}'\n",
    "print(f'We have the following conversations: {list(problem.conversations.keys())}')\n",
    "key, conversation = next(iter(problem.conversations.items()))  # get a key-value pair from the dictionary\n",
    "print(f'Displaying conversation: {key}')\n",
    "for message in conversation.messages:  # conversation.messages is a list of champ_dataset.Message instances\n",
    "    # one of 'System', 'User', 'Imputation' and 'Model.*' (e.g., 'Model.GPT-3.5')\n",
    "    print(f'Role: {message.role}')\n",
    "    # verbatim content of the message\n",
    "    print(f'Text: {message.text}')\n",
    "    # any possible error with the model-generation\n",
    "    print(f'Error: {message.error}')\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47033a2c-c307-4314-968c-100093d909e2",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab9a95c-ee97-4fea-b602-e9a80cd565c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_math_type = ['Combinatorics','Inequality','Number-Theory','Polynomial','Sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78edf207-b027-4da5-9145-27e593e623bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMT = \"\\nRespond in a single word. In this five catagories: 'Combinatorics','Inequality','Number-Theory','Polynomial','Sequence'.What is the catagory for this math problem?\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33c419b-9538-44f6-8e03-ff71c03ade01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270    270\n"
     ]
    }
   ],
   "source": [
    "problem_type_list = []\n",
    "problem_text_list = []\n",
    "\n",
    "for i in dataset.problems.keys():\n",
    "    problem = dataset[i]\n",
    "    current_promt = problem.text.replace('?', '') + PROMT\n",
    "    current_promt = problem.text\n",
    "    problem_text_list.append(current_promt)\n",
    "    for current_type in list_math_type:\n",
    "        if current_type in problem.identifier:\n",
    "            problem_type_list.append(current_type)\n",
    "            break\n",
    "\n",
    "\n",
    "print(len(problem_type_list), '  ', len(problem_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d97cca1-cff4-434d-80b2-03d31ab53e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let a sequence be the defined as a_1=a_2=1, and a_n=(a_(n-1)^2+2)/a_(n-2). How many values in a_1, a_2, ..., a_100 are integers?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d9ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "problem_type_list = encoder.fit_transform(np.array(problem_type_list).reshape(-1,1))\n",
    "#problem_text_list = np.array(problem_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "938b93cc-25ac-4660-bc82-778d63811d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 216\n",
      "Test set size: 54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(problem_text_list, problem_type_list, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "050e028c-87ca-4ad0-9612-4147523b90d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24fa2ad3c2044c6b6e7c1476380bd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, FlaxLlamaModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216a019b-1d17-4317-b83a-4e14a389f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/home/qdang1/.conda/envs/tf2/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I hope this email finds you well. I am writing to inquire\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74a24b87-fc9f-48cf-8e91-2e3f965f8073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 8.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33c9fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers of the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Fine-tune only the last transformer block (layer)\n",
    "# for param in model.transformer.h[-1].parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fce4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Use the hidden state of the first token (usually the [CLS] token)\n",
    "        cls_output = hidden_states[:, 0, :]\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "\n",
    "# Number of classes in the classification task\n",
    "num_classes = 5\n",
    "\n",
    "# Add classification head on top of the Llama model\n",
    "classification_head = ClassificationHead(input_size=model.config.hidden_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5d1edc-8a5e-44c8-bdf3-359581e064ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Llama model and classification head\n",
    "class LlamaForClassification(nn.Module):\n",
    "    def __init__(self, llama_model, classification_head):\n",
    "        super(LlamaForClassification, self).__init__()\n",
    "        self.llama_model = llama_model\n",
    "        self.classification_head = classification_head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Get the hidden states from the Llama model\n",
    "        outputs = self.llama_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Last hidden state\n",
    "        \n",
    "        # Pass the hidden states to the classification head\n",
    "        logits = self.classification_head(hidden_states)\n",
    "        return logits\n",
    "\n",
    "# Create the combined model\n",
    "model_for_classification = LlamaForClassification(model, classification_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf37f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Convert y_train and y_test to tensors\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model_for_classification.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6d228d4-9875-4659-bbda-83f17623b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca8ac5cf-e971-4746-be96-0e6a5dc6d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the train and test data\n",
    "train_encodings = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(X_test, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get input ids and attention masks\n",
    "train_input_ids = train_encodings['input_ids']\n",
    "train_attention_mask = train_encodings['attention_mask']\n",
    "\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_attention_mask = test_encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2adf4c34-6f04-413f-912d-993bf14379db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create TensorDatasets for training and test sets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, y_train)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, y_test)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da8cea87-d378-4879-9f66-d2c53207a339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "Epoch 1, Loss: 1.638376304081508\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "Epoch 2, Loss: 1.6477112770080566\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "Epoch 3, Loss: 1.6039048773901803\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "Epoch 4, Loss: 1.6411420617784773\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "batch\n",
      "Epoch 5, Loss: 1.5887463773999895\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model_for_classification.train()\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        print('batch')\n",
    "        # Unpack batch\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model_for_classification(batch_input_ids, batch_attention_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd150ee5-4029-4d74-bfa6-bd9ca4e0f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "batch\n",
      "Test Predictions: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "model_for_classification.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        print('batch')\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "        logits = model_for_classification(batch_input_ids, batch_attention_mask)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "# Convert predictions to a single tensor\n",
    "all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "print(f\"Test Predictions: {all_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba62123f-bc20-4fb3-a4f1-05afa0942287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 2, 3, 2, 3, 1, 2, 4, 3, 4, 4, 1, 2, 3, 4, 2, 2, 1, 4, 4, 1, 1,\n",
       "        3, 0, 0, 3, 2, 0, 2, 1, 2, 4, 4, 0, 2, 1, 2, 2, 0, 2, 4, 0, 1, 3, 2, 3,\n",
       "        2, 3, 0, 2, 4, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(y_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629bda4-b4c3-45fc-8eeb-06798fbec658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fefc29-4d56-4688-ab60-6ce94b64fd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd8d12-f423-43c3-b7b6-d6532689bf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68bf36-8399-442a-a948-7ec04573f572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd77a2b-4c5b-4bae-a0fe-e5010e873cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000f615-fb9b-462f-94d7-71635fb0e6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45375332-df05-4aff-bca1-cdf408857ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
